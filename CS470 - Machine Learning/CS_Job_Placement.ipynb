{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS_Job_Placement.ipynb","provenance":[],"collapsed_sections":["bYxsrpl_QriL","yyxbe8W2Qri1","Ee-4-8VnQrjB","COI29UDEQrjn","bfVJHz_0C8X6"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gaBsQSjDhRug"},"source":["## CSCI 470 Activities and Case Studies\n","\n","1. For all activities, you are allowed to collaborate with a partner. \n","1. For case studies, you should work individually and are **not** allowed to collaborate.\n","\n","By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."]},{"cell_type":"markdown","metadata":{"id":"7Ddz9Y6ihRum"},"source":["Some considerations with regard to how these notebooks will be graded:\n","\n","1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n","2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n","2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n","3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n","4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n","5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n","6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n","7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n","8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"oTFUB2ZSQriJ","nbgrader":{"cell_type":"markdown","checksum":"84752e8eb4150e638e2e69112e22fa12","grade":false,"grade_id":"cell-c95444a9726cb6ad","locked":true,"schema_version":3,"solution":false}},"source":["# Case Study: Job Placement\n","\n","### Predicting job performance of candidate hires\n","\n","You work for a software startup, Predict All The Things Inc. (PALT), and are approached by the CEO to build an algorithm that can help sift through resumes. PALT just closed a $3 million Series A round of funding and the CEO just landed a deal with a national retailer, SellsALOT, to help them with hiring Sales Associates.\n","\n","They are able to obtain data on all the employees that work as Sales Associates throughout their stores as well as customer satisfaction and sales performance scores.\n","\n","In this case study, you are tasked with building a model to predict job performance to assist HR in selecting applicants to interview.\n","\n","The data was provided to you by the new HR intern, Keegan. This is the email you got from Keegan with the attached data.\n","\n",">Hi!\n",">\n",">I hope you're doing well. I've attached the data we have about all employees. Please ensure this data stays confidential and is not shared with anyone who has not signed the NDA. The columns have all the information we have about our employees and the scoring rating that they've received from our performance monitors. We also have some employees who were fired and I have included those as well.\n",">\n",">I was also able to dig up some more information about our employees that I found on the internet. It took a lot of time but I hope it helps in making the model even better. Can't wait to see this thing in action. Everyone here is very excited about our collaboration with you and we look forward to this making hiring a lot easier for us.\n",">\n",">Thanks,\n",">\n",">Keegan Thiel\n",">\n",">HR Intern\n",">\n",">Human Resources\n",">\n",">SellsALOT\n","\n","\n","Data is available in the `employees.csv` file provided. \n","\n","\n","SellsALOT is an Equal Opportunity Employer which is an employer who agrees not to discriminate against any employee or job applicant based on race, color, religion, national origin, sex, physical or mental disability, or age.\n"]},{"cell_type":"markdown","metadata":{"id":"I-0GwhMLhRup"},"source":["### Part 1: Data Cleaning\n","\n","In part 1 of this notebook you will explore the data, manually decide which features you will keep for your models, and convert all features into numeric values that can be ingested by ML models.\n","\n","By the __Part 1 deadline__, you will need to complete the data cleaning, save your cleaned data as a new CSV file, and __submit that CSV file in Canvas for grading__.\n","\n","\n","### Part 2: Modeling\n","\n","In Part 2 you will build, train, and evaluating (on the test set) six different models.  \n","- __Interviewing__: You will build three models that are meant for selecting candidates for job __interviews__. Each model has a different target but takes in the same features.\n","- __Hiring__: You will build three additional models that are meant for selecting __direct hire__ candidates. Like the interviewing models, each model has a different target but the same input features.\n","\n","After evaluating you individual models, you'll then create a new candidate-scoring function (not a model-scoring function) that combines all three predictions into a scalar value that can be used to rank applicants for interviewing, or for hiring.\n","\n","Finally, you'll use data from some \"new\" applicants, as well as create some of you own, and observe your models' predictions for that data.\n","\n","By the __Part 2 deadline__ you'll need to finalize and submit this notebook.\n","\n","### Grading\n","\n","- The Part 1 CSV file is worth __20 points__.  \n","- This notebook, both the Data Cleaning and Modeling sections together, is worth __80 points__.  \n","- The case study is worth __100 points in total__."]},{"cell_type":"markdown","metadata":{"id":"QCU4JRRt7i44"},"source":["## Import packages that are likely to be useful\n","\n","### However, __do not__ use TensorFlow to build your models.\n","\n","Below we import packages that are needed or may be useful. You may import additional packages as you see fit, with the exception of TensorFlow. You may use scikit-learn. **Ensure that you import additional packages in cells that say \"### YOUR CODE HERE\".**"]},{"cell_type":"code","metadata":{"id":"oDynCtWlQriO","nbgrader":{"grade":false,"grade_id":"cell-415e7e827876c782","locked":false,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086382452,"user_tz":420,"elapsed":281,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["import pandas as pd\n","import matplotlib\n","import numpy as np\n","import sklearn as sk\n","from sklearn.model_selection import train_test_split\n","import datetime\n","from datetime import date\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"bYxsrpl_QriL","nbgrader":{"cell_type":"markdown","checksum":"36fb3abb8b9d714d711c318557f98208","grade":false,"grade_id":"cell-f9469a23a33f582b","locked":true,"schema_version":3,"solution":false}},"source":["## Data Cleaning\n","\n","First, let's investigate the data that we received from Keegan.\n","\n","If you are using colab, **Make sure you upload the employees.csv file** so it can be loaded in the next cell. In order to do so, click the file folder icon in the colab sidebar. You will see the contents of the current directory, which will include a \"sample_data\" folder. Click the upload icon (piece of paper with upward pointing arrow, just below the word \"Files\"). Locate the employees.csv file that you downloaded from Canvas to your local machine and open/upload the file."]},{"cell_type":"code","metadata":{"id":"EIfgJtszQriZ","nbgrader":{"grade":false,"grade_id":"cell-2e0e3f39a04d036d","locked":false,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086384676,"user_tz":420,"elapsed":140,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["df = pd.read_csv(\"employees.csv\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHHFixaFQrif"},"source":["df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xgby0qLjQrio"},"source":["df.describe(include=\"all\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"53QCOLGNQriu"},"source":["print(\"The columns of data are:\")\n","list(df.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"bwpWFM4JSI1n","nbgrader":{"cell_type":"markdown","checksum":"de79ab46da1ec15d40873fd7c93e4a87","grade":false,"grade_id":"cell-da9afb0de2933b89","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Before building any models, your manager has asked you to **convert all the feature data into formats that can easily be used for training and testing a variety of models**. This means:\n","1. Splitting the 16 Myers Briggs types into 4 subtypes\n","2. Converting categorical features into dummy binary features\n","3. Calculating age based on date of birth\n","4. Dealing with missing (NaN) values in the data\n","\n","In addition, you should remove columns that contain redundant information after going through the process above, e.g., removing the 'Date of Birth' column after an 'Age' column is added.\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"yyxbe8W2Qri1","nbgrader":{"cell_type":"markdown","checksum":"de5ac398f50372aa07832a78fa2f71ec","grade":false,"grade_id":"cell-759ebf896978ca5d","locked":true,"schema_version":3,"solution":false}},"source":["### MBTI Splitting\n","\n","The [Myers Briggs Type Indicator](https://en.wikipedia.org/wiki/Myers%E2%80%93Briggs_Type_Indicator) (MBTI) descibes people as one of two types for each of:\n","\n","* extraversion (E) or introversion (I)\n","* sensing (S) or intuition (N)\n","* thinking (T) or feeling (F)\n","* judgment (J) or perception (P)\n","\n","It would make more sense for us to represent people as one or the other of these instead of creating all the possible cases. That way a model can learn based on each of those factors as well as their combination. \n","\n","Your next task is to split the MBTI column into four columns in the dataframe, with the following column names and values:\n","\n","* MBTI_EI with value `E` or `I`\n","* MBTI_SN with value `S` or `N`\n","* MBTI_TF with value `T` or `F`\n","* MBTI_JP with value `J` or `P`\n","\n","that correspond to the same row's Myers Briggs Type, and add those columns to your DataFrame, ```df```. Consider using the Series ```apply()``` method.\n","\n","Afterwards, `drop` (remove) the original \"Myers Briggs Type\" column."]},{"cell_type":"code","metadata":{"deletable":false,"id":"IKa0UuiRQri2","nbgrader":{"cell_type":"code","checksum":"684f43b481383ae1fa1488100efe524b","grade":false,"grade_id":"cell-b96d43f098c26707","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086395413,"user_tz":420,"elapsed":662,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# YOUR CODE HERE\n","df[['MBTI_EI', 'MBTI_SN', 'MBTI_TF', 'MBTI_JP']] = df['Myers Briggs Type'].apply(lambda x: pd.Series(list(x)))\n","df = df.drop(columns='Myers Briggs Type')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"5UPBPY9BQri8","nbgrader":{"cell_type":"code","checksum":"a808a0fd0697bf97ca1b5549b1c77ae6","grade":true,"grade_id":"cell-1b5cf11c2512def1","locked":true,"points":2,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086397238,"user_tz":420,"elapsed":139,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(set(df[\"MBTI_EI\"])) == 2\n","assert \"E\" in set(df[\"MBTI_EI\"]) and \"I\" in set(df[\"MBTI_EI\"])\n","assert len(set(df[\"MBTI_SN\"])) == 2\n","assert \"S\" in set(df[\"MBTI_SN\"]) and \"N\" in set(df[\"MBTI_SN\"])\n","assert len(set(df[\"MBTI_TF\"])) == 2\n","assert \"T\" in set(df[\"MBTI_TF\"]) and \"F\" in set(df[\"MBTI_TF\"])\n","assert len(set(df[\"MBTI_JP\"])) == 2\n","assert \"J\" in set(df[\"MBTI_JP\"]) and \"P\" in set(df[\"MBTI_JP\"])\n","assert \"Myers Briggs Type\" not in list(df.columns)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Rp7NQOOZQrjA","nbgrader":{"cell_type":"markdown","checksum":"d0d66ae39d5eba1acfed519336f95590","grade":false,"grade_id":"cell-51a7104ebad2aa72","locked":true,"schema_version":3,"solution":false}},"source":["1. ~~Splitting the 16 Myers Briggs types into 4 subtypes~~\n","2. Converting categorical features into dummy binary features\n","3. Calculating age based on date of birth\n","4. Dealing with missing (NaN) values in the data"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Ee-4-8VnQrjB","nbgrader":{"cell_type":"markdown","checksum":"10408ba0fed6f9148984ec20dd90ee82","grade":false,"grade_id":"cell-364e01542d424264","locked":true,"schema_version":3,"solution":false}},"source":["### Categorical to Dummy Variables\n","\n","Dummy variables are variables that allow us to convert a category into several binary variables. For example, if we had a color value that we were storing and we knew it could only have the values `red`, `green`, and `blue`, then instead of storing the color as those strings, we can store three binary variables: `is_red`, `is_green`, and `is_blue`. This is identical to \"one-hot\" encoding.\n","\n","We can do this in pandas easily by using [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)."]},{"cell_type":"code","metadata":{"deletable":false,"id":"NC0DysIpQrjB","nbgrader":{"cell_type":"code","checksum":"e13d88bbab2f3016e5fb351d21ec0059","grade":false,"grade_id":"cell-32271c292a515ef3","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086399413,"user_tz":420,"elapsed":136,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Review the DataFrame columns and identify the columns that contain categorical\n","# features and save them to a list called \"categorical_columns\". Afterwards\n","# we will convert those features to one-hot encoded features, using get_dummies().\n","\n","# \"Categorical\" means that there is a discrete (albeit large in some cases)\n","# number of possible options, and that those values have no ordinal (rankable)\n","# meaning.\n","# A slightly tricky one is zip codes. Are zip codes ordinal? Zip codes do increase,\n","# generally, as one moves from east to west in the US. But there is otherwise\n","# little ordinal relationship, so you may treat Zipcode as a categorical variable for\n","# this case study if you wish to use it as a feature for your ML models.\n","\n","# While the following are technically categorical, they should have little to no\n","# predictive power, so don't include them in your list of categorical columns since\n","# they shouldn't be used as features for your ML model in any case.\n","#     'First Name'\n","#     'Last Name'\n","#     'Address'\n","    \n","# YOUR CODE HERE\n","categorical_columns = ['Zipcode', 'Gender', 'Race / Ethnicity', 'English Fluency', 'Spanish Fluency', 'Education', 'MBTI_EI', 'MBTI_SN', 'MBTI_TF', 'MBTI_JP', 'Requires Sponsorship', 'Fired']\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"jdNtmrl6QrjH","nbgrader":{"cell_type":"code","checksum":"b8d7ed6806dc9b1028bad97d5f57c769","grade":true,"grade_id":"cell-528eea88864f2926","locked":true,"points":2,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086401731,"user_tz":420,"elapsed":138,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(categorical_columns) > 8\n","for category in categorical_columns:\n","    assert category in df.columns"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-LVPXKCE5rH","nbgrader":{"grade":false,"grade_id":"cell-526438097a7b2b8d","locked":false,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086404078,"user_tz":420,"elapsed":131,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Before we get the dummy variables, we need to make sure that all these \n","# categorical columns are actually recognized by pandas to be of 'category' type.\n","\n","for column in categorical_columns:\n","    df[column] = df[column].astype('category')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"9nXNaoYeQrjL","nbgrader":{"cell_type":"code","checksum":"3990d21bd359541639089bd76c03c56b","grade":false,"grade_id":"cell-ad307669e4346527","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086405501,"user_tz":420,"elapsed":183,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# For every column in the categorical_columns,\n","# calculate the dummy variables and add them to the dataframe\n","\n","# YOUR CODE HERE\n","df = pd.get_dummies(df, columns=categorical_columns)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wjry6pPNQrjQ","nbgrader":{"grade":false,"grade_id":"cell-d477e9bead9db0a6","locked":false,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086406455,"user_tz":420,"elapsed":142,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(list(df.columns)) > 45"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYs2UHNUQrjU","nbgrader":{"grade":false,"grade_id":"cell-4b43f9bb988091cf","locked":false,"schema_version":3,"solution":false}},"source":["print(\"The current columns are:\")\n","list(df.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"2DeBsotIQrja","nbgrader":{"cell_type":"code","checksum":"de4234ba9e730dc149e2e86a3a4e14fd","grade":false,"grade_id":"cell-10890eae4243cb8d","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086410552,"user_tz":420,"elapsed":153,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Now drop all the categorical features columns (those in your\n","# \"categorical_columns\" list) from the dataframe, so that we don't\n","# have duplicate information stored.\n","\n","# YOUR CODE HERE\n","# you may (read: shouldn't ?) need to even do this? But for clarity: \n","# Yeah it actually doesn't even run once, I was moderately shocked by this.\n","# df.drop(columns=categorical_columns)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbjQdgc2Qrje","nbgrader":{"grade":false,"grade_id":"cell-c0598d253d832ddd","locked":false,"schema_version":3,"solution":false}},"source":["print(\"The current columns are:\")\n","list(df.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"m44-1XHyQrjj","nbgrader":{"cell_type":"code","checksum":"11545599f9547d63717d998ed9a5cd1f","grade":true,"grade_id":"cell-a6172310793c3289","locked":true,"points":2,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086414748,"user_tz":420,"elapsed":151,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert 55 > len(list(df.columns)) > 30"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"COI29UDEQrjn","nbgrader":{"cell_type":"markdown","checksum":"77c1e8cf92655b525a94d6cb6a39c945","grade":false,"grade_id":"cell-48ebecdc645e8f6a","locked":true,"schema_version":3,"solution":false}},"source":["1. ~~Splitting the 16 Myers Briggs types into 4 subtypes~~\n","2. ~~Converting categorical features into dummy binary features~~\n","3. Calculating age based on date of birth\n","4. Dealing with missing (NaN) values in the data\n","\n","### Age Calculation"]},{"cell_type":"code","metadata":{"id":"47ydwvFeQrjo","nbgrader":{"grade":false,"grade_id":"cell-028aecaf0638bb7a","locked":false,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086418078,"user_tz":420,"elapsed":131,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["def calculate_age(born):\n","    \"\"\"Calculates age (in years) based on date of birth\n","       using https://stackoverflow.com/a/9754466/818687\n","\n","    Args:\n","        born (datetime): The date of birth\n","\n","    Returns:\n","        int: The age based on date of birth\n","    \"\"\"\n","    \n","    # We'll set a fixed date for \"today\" rather then use the actual date,\n","    # so the data will be the same regardless of when you run this notebook.\n","    today = datetime.datetime.strptime(\"2021-11-20\", \"%Y-%m-%d\")\n","    \n","    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLuFWRZaQrjs"},"source":["Add an \"Age\" column to the dataframe, with the help of the ```calculate_age()``` function above. Afterwards, remove the \"Date of Birth\" column.\n","\n","The input to ```calculate_age()``` should be a datetime object. Review the ```datetime.datetme.strptime()```\n","function and [format codes](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior) to determine how to convert the \"Date of Birth\" date string into a `datetime` object. Take note that __capitalization matters__ in the format code, e.g., the difference between (m)onth and (M)inute."]},{"cell_type":"code","metadata":{"deletable":false,"id":"QI7eW-FoQrjs","nbgrader":{"cell_type":"code","checksum":"f8aa6b8b6f87c947c924722d60becd56","grade":false,"grade_id":"cell-315ed06d1f0428b4","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086420224,"user_tz":420,"elapsed":154,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# YOUR CODE HERE\n","df['Date of Birth'] = pd.to_datetime(df['Date of Birth'])\n","df[['Age']] = df['Date of Birth'].apply(lambda x: calculate_age(x))\n","df = df.drop(columns='Date of Birth')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"2xcfEfy9Qrjv","nbgrader":{"cell_type":"code","checksum":"1ef7abcf578f44548f2c5c7add8cf5cf","grade":true,"grade_id":"cell-abfc46abea98fba7","locked":true,"points":4,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086421051,"user_tz":420,"elapsed":142,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert df[\"Age\"].min() == 23\n","assert df[\"Age\"].max() == 39\n","assert np.isclose(df[\"Age\"].median(), 31)\n","assert \"Date of Birth\" not in list(df.columns)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bfVJHz_0C8X6"},"source":["1. ~~Splitting the 16 Myers Briggs types into 4 subtypes~~\n","2. ~~Converting categorical features into dummy binary features~~\n","3. ~~Calculating age based on date of birth~~\n","4. Dealing with missing (NaN) values in the data\n","\n","## Handle NaN values"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPtjdTXjDVOr","executionInfo":{"status":"ok","timestamp":1639086422638,"user_tz":420,"elapsed":136,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"591b17c7-743d-4b3d-de69-2b49258b42c1"},"source":["# Create a list of columns that contain NaN values\n","\n","nan_columns = df.columns[df.isna().any()].tolist()\n","print(nan_columns)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["['High School GPA', 'College GPA']\n"]}]},{"cell_type":"markdown","metadata":{"id":"A7-lkXhdDmkO"},"source":["We see that data is not truly \"missing\" any values, but for people that did not attend or complete high school or college, there are no GPA values.\n","\n","How should you deal with this? If you had a large number of people without GPAs, you might consider making separate models for people with GPAs and for people without. For this case, your manager asks you to make sure there's one model for everyone. She recommends one of the following options:\n","\n","1. Replace NaN values with the mean value of all the non-NaN values.\n","1. Replace NaN values with 0\n","1. Replace NaN values with some other value\n","1. Create a model to predict people's GPA values from other attributes and fill them in with those values\n","\n","Consider the assumptions of each approach:\n","1. Replacing with the mean assumes that that person would receive the average of others who work at this company.\n","1. Replacing with 0 assumes that that person would fail if they attended high school or college.\n","1. Replacing with some arbitrary value will have assumptions based on what that value is\n","1. Creating a model to predict people's GPA values from the other attributes in the data assumes that those attributes are predictive of GPA. \n","\n","\n","Regardless of the approach you take, just make sure there are no more NaN values. "]},{"cell_type":"code","metadata":{"deletable":false,"id":"ZKJLKs2xE06h","nbgrader":{"cell_type":"code","checksum":"fd73b92aa5b1a069c528dabd348caa29","grade":false,"grade_id":"cell-5b544815d1fa2e03","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1639086424899,"user_tz":420,"elapsed":251,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# For each of the two columns that contain NaN values, replace the NaN values\n","# with numerical values, using one of the approaches above, or some other approach\n","# that you devise yourself.\n","\n","# YOUR CODE HERE\n","# you can do in 1 line using df.fillna(0) but I didn't want to make an error in haste\n","df['High School GPA'] = df['High School GPA'].fillna(0)\n","df['College GPA'] = df['College GPA'].fillna(0)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"bAi-bCIZFOp9","nbgrader":{"cell_type":"code","checksum":"71a9c2e0540b66df838ef99a2b9b1286","grade":true,"grade_id":"cell-06ba496085783976","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086426159,"user_tz":420,"elapsed":199,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["for col in nan_columns:\n","    assert not df[col].isna().any()"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9dde5ca770d5a332fb3ecfb9d327b60f","grade":false,"grade_id":"cell-7bebafd4a8e5d502","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"xvLM70HbhRu3","executionInfo":{"status":"ok","timestamp":1639086427121,"user_tz":420,"elapsed":327,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"df596184-5371-423d-e645-da09eeadafce"},"source":["# Describe the approach you chose and why and save that as a string called nan_filling_approach\n","\n","# YOUR CODE HERE\n","nan_filling_approach = '''I filled the NAN values with 0, because if they didnt have a score it may not exactly be genuine to assign an average. This may be critical (of people) but its also theoretical so I\n","didn't care to dwell on it. Depending on the job perhaps this is not a good indicator at all and can be left out.'''\n","print(nan_filling_approach)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["I filled the NAN values with 0, because if they didnt have a score it may not exactly be genuine to assign an average. This may be critical (of people) but its also theoretical so I\n","didn't care to dwell on it. Depending on the job perhaps this is not a good indicator at all and can be left out.\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b8d7628c6a95a53b9ee25dcbec20cd5b","grade":true,"grade_id":"cell-e58a09c92fb5074b","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"yevmK7XQhRu4","executionInfo":{"status":"ok","timestamp":1639086428363,"user_tz":420,"elapsed":195,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(nan_filling_approach) > 30"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbxoW3I4hRu4"},"source":["## Save your cleaned data and submit it by the Part 1 deadline\n","\n","Uncomment the code in the cell below and use it to save your DataFrame of cleaned data to a .csv file.  \n","Afterwards you can re-comment or delete the code if you like."]},{"cell_type":"code","metadata":{"id":"IYgGg1KWhRu4","executionInfo":{"status":"ok","timestamp":1639086429614,"user_tz":420,"elapsed":160,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["## Uncomment the line below to save your data to a .csv file, for Part 1 submission.\n","\n","df.to_csv('employees_cleaned.csv', header=True, index=False)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k_52XMEQHMfA"},"source":["## Modeling"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"5Ehsr1FaQrjz","nbgrader":{"cell_type":"markdown","checksum":"5ae89aed737a6106f40120d8fce81e5a","grade":false,"grade_id":"cell-ea0c08afcc388389","locked":true,"schema_version":3,"solution":false}},"source":["### Interviewing model(s)\n","\n","Having completed the conversion of the data into a format that can be used with machine learning models, your manager asks that you build three seperate models which predict the following three targets, respectively:\n","\n","1. Customer Satisfaction\n","1. Sales Performance\n","1. Fired"]},{"cell_type":"code","metadata":{"deletable":false,"id":"KC0mGpc2Qrj0","nbgrader":{"cell_type":"code","checksum":"acb83f73e1e3266297a795ba153c774e","grade":false,"grade_id":"cell-449eb7f2253cca83","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086431192,"user_tz":420,"elapsed":167,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Save the names of columns we are trying to predict to a list called \"targets\".\n","# Make sure that if we had a categorical column, that you use the dummy representation(s).\n","\n","# YOUR CODE HERE\n","# df.head()\n","targets = ['Customer Satisfaction Rating', 'Sales Rating', 'Fired_Fired']"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"TDOU2C7ZQrj4","nbgrader":{"cell_type":"code","checksum":"e888ed217d50a36bfd2cb6d0acbd90ed","grade":true,"grade_id":"cell-458aa3767005cc6d","locked":true,"points":4,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086432620,"user_tz":420,"elapsed":249,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(targets) == 3\n","for target in targets:\n","    assert target in df.columns"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"sQ3mnuWzQrj7","nbgrader":{"cell_type":"markdown","checksum":"8fa2f0ce39bc630d40108d78c6c9ee69","grade":false,"grade_id":"cell-001c7adb39caf635","locked":true,"schema_version":3,"solution":false}},"source":["Ultimately, the predictions of your models will be used to rank applicants for interviews with HR.\n","\n","**Which features will you select to use in your interview ranking model?** You will specify them below."]},{"cell_type":"code","metadata":{"id":"bJ1bAdziQrj8","nbgrader":{"grade":false,"grade_id":"cell-e436c6aea590b5c0","locked":false,"schema_version":3,"solution":false}},"source":["print(\"The available columns are:\")\n","list(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"_Q71StrCQrkB","nbgrader":{"cell_type":"code","checksum":"2304b12253a23a6ab1f3a6409f8de029","grade":false,"grade_id":"cell-bd2399d20615cbd1","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086436579,"user_tz":420,"elapsed":137,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Enter all the features you want to use in a list and save it to \"interview_features\".\n","# These are the features for the models that will predict the targets, and the\n","# predictions will be used to rank applicants for **interviews**.\n","\n","# YOUR CODE HERE\n","interview_features = [\n","                      'Years of Experience', \n","                      'Years of Volunteering',\n","                      'High School GPA',\n","                      'College GPA',  \n","                      'English Fluency_Basic',\n","                      'English Fluency_Fluent',\n","                      'English Fluency_Proficient',\n","                      'Spanish Fluency_Basic',\n","                      'Spanish Fluency_Fluent',\n","                      'Spanish Fluency_Proficient',\n","                      'Education_Associates',\n","                      'Education_Graduate',\n","                      'Education_Undergraduate',\n","                      'MBTI_EI_E',\n","                      'MBTI_EI_I',\n","                      'MBTI_SN_N',\n","                      'MBTI_SN_S',\n","                      'MBTI_TF_F',\n","                      'MBTI_TF_T',\n","                      'MBTI_JP_J',\n","                      'MBTI_JP_P',\n","                      ]"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"IEjtob_PQrkE","nbgrader":{"cell_type":"markdown","checksum":"734ba0669bdc3873fb75b67ef74cc737","grade":false,"grade_id":"cell-c35210aa72f0c989","locked":true,"schema_version":3,"solution":false}},"source":["Why did you choose the features you did?"]},{"cell_type":"code","metadata":{"deletable":false,"id":"szA3hUTVQrkE","nbgrader":{"cell_type":"code","checksum":"8e708328365011b9596ddff4eef1a7db","grade":false,"grade_id":"cell-11725eab2a5dc436","locked":false,"schema_version":3,"solution":true},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086439075,"user_tz":420,"elapsed":124,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"e1712a30-fa71-4426-ec0a-f5281f5f8057"},"source":["## Save your reasoning in a string to the variable interview_reason\n","\n","# YOUR CODE HERE\n","interview_reason = '''In general I thought YOE/Volunteer History could be useful, as well as general language, education, and personality metrics. I stayed away from superficial things like \n","race/gender, etc. Generally an interview is to ask questions about other things IDK. I picked these because they are telling of a candidate but also things that are controlled (mostly).'''\n","print(interview_reason)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["In general I thought YOE/Volunteer History could be useful, as well as general language, education, and personality metrics. I stayed away from superficial things like \n","race/gender, etc. Generally an interview is to ask questions about other things IDK. I picked these because they are telling of a candidate but also things that are controlled (mostly).\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"IjjWNMcdQrkH","nbgrader":{"cell_type":"code","checksum":"f41989d48f655180f854666704a76e46","grade":true,"grade_id":"cell-8b0e41f067c8ef32","locked":true,"points":4,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086441593,"user_tz":420,"elapsed":121,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert isinstance(interview_reason, str)\n","assert len(interview_reason) > 20"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"uro6564gQrkL","nbgrader":{"cell_type":"code","checksum":"a28db3b94c4012687215531c125259c8","grade":false,"grade_id":"cell-d9d5cf91c533af77","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086450639,"user_tz":420,"elapsed":162,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Perform a train and test split on the data with the variable names:\n","#\n","# interview_x_train for the training features\n","# interview_x_test  for the testing features\n","#\n","# interview_y_train for the training targets\n","# interview_y_test  for the testing targets\n","#\n","# The test dataset should be 20% of the total dataset\n","\n","# YOUR CODE HERE\n","# I saw on piazza that you generally avoid using the entire df[] in the split, but it seemed to work OK? (probably just best practice to make variables for them... naming-wise even)\n","interview_x_train, interview_x_test, interview_y_train, interview_y_test = train_test_split(df[interview_features], df[targets], test_size=0.2)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"LMrRXdYnQrkN","nbgrader":{"cell_type":"code","checksum":"7243260f608bebe999dd6c9d4a6699ed","grade":true,"grade_id":"cell-146c7b1df2f26d90","locked":true,"points":2,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086453058,"user_tz":420,"elapsed":138,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert (len(interview_x_train) / (len(interview_x_test) + len(interview_x_train))) == 0.8\n","assert (len(interview_y_train) / (len(interview_y_test) + len(interview_y_train))) == 0.8\n","assert len(interview_x_train) == len(interview_y_train)\n","assert len(interview_x_test) == len(interview_y_test)"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkHr4ehrvnqa"},"source":["Build and train your interviewing models."]},{"cell_type":"code","metadata":{"deletable":false,"id":"7nJGrXs8QrkQ","nbgrader":{"cell_type":"code","checksum":"1ed756e81a6658a8a79716574b0a2ef9","grade":true,"grade_id":"cell-99742e48e538f29a","locked":true,"points":10,"schema_version":3,"solution":true}},"source":["# Select models of your choosing, import them here, and perform a\n","# hyperparameter search while training them on each of the targets.\n","#\n","# Do not use Tensorflow to build a model - you may use scikit-learn.\n","#\n","# Determine an appropriate metric for measuring your performance for each\n","# model/target, and report the test score for that metric. The metric may be\n","# different for each model/target.\n","#\n","# Save your models in a list, with models ordered in the same manner as the\n","# targets they are predicting in the list \"targets\" you created above.\n","# Call the list \"my_interview_models\", e.g.\n","#\n","#    my_interview_models = [interview_model_target1,\n","#                           interview_model_target2,\n","#                           interview_model_target3]\n","#\n","# You should use multiple print messages to print something like the\n","# following for each of your models/targets:\n","#\n","#    To predict the target (target), I trained a (model) model\n","#    and determined the best hyperparameters as (param1 = p1), (param2 = p2), ...\n","#    resulting in a (metric) score of (score).\n","\n","# YOUR CODE HERE\n","# targets = ['Customer Satisfaction Rating', 'Sales Rating', 'Fired_Fired']\n","# My initial thoughts are to just use a ridge model for all 3.\n","model = Ridge()\n","\n","# GridSearch & MSE Calculations:\n","parameters = {'alpha':[1, 10]}\n","interview_scores = []\n","my_interview_models = []\n","\n","for t in targets:\n","  # search & fit\n","  search = GridSearchCV(model, parameters, scoring='neg_mean_squared_error', cv=5)\n","  search.fit(interview_x_train, interview_y_train[t])\n","  # MSE calculations + print statement. \n","  best_esti = search.best_estimator_\n","  pred = best_esti.predict(interview_x_test)\n","  mse = mean_squared_error(interview_y_test[t], pred)\n","  interview_scores.append(mse)\n","  my_interview_models.append(best_esti)\n","  output = output = 'To predict the target, Customer Satisfaction Rating, I trained a Ridge model and determined the best hyperparameter to be alpha = {}, resulting in an MSE score of {}'.format(search.best_estimator_, mse)\n","  print(output)\n","\n","# List of models to use in the skeleton code\n","print(len(my_interview_models))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkdngUX7yMJq","executionInfo":{"status":"ok","timestamp":1639086459017,"user_tz":420,"elapsed":125,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(my_interview_models)==len(targets)"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"-4WvDVFYQrkT","nbgrader":{"cell_type":"markdown","checksum":"d909350ea6f55a3151827ad93b357df0","grade":false,"grade_id":"cell-be0de61adb8d2930","locked":true,"schema_version":3,"solution":false}},"source":["### Hiring model(s)\n","\n","You manager tells you that SellsALOT has decided they wish to consider doing away with interviews altogether, in order to save money. SellsALOT would like a model that will be used to rank candidates for directly hiring them, rather than for interviewing them.\n","\n","Will your choice of features changes?\n","\n","**Which features will you select to use in that model?** You will specify them below."]},{"cell_type":"code","metadata":{"id":"k2a4ceZsQrkU","nbgrader":{"grade":false,"grade_id":"cell-3aaedae380c7db79","locked":false,"schema_version":3,"solution":false}},"source":["print(\"The available columns are:\")\n","list(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"ZRyhvmMaQrkW","nbgrader":{"cell_type":"code","checksum":"aac9ee0ef0cb43dddc721d66f66eafc6","grade":false,"grade_id":"cell-c4bad209682c7022","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086463520,"user_tz":420,"elapsed":153,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Enter all the features you want to use in a list and save it to \"hire_features\".\n","# These are the features for the models that will predict the targets, and the\n","# predictions will be used to rank applicants for **hiring**.\n","\n","# YOUR CODE HERE\n","hire_features = [\n","                      'Zipcode_24310',\n","                      'Zipcode_30167',\n","                      'Zipcode_43357',\n","                      'Zipcode_43711',\n","                      'Zipcode_54821',\n","                      'Zipcode_55864',\n","                      'Zipcode_59010',\n","                      'Zipcode_60531',\n","                      'Zipcode_72361',\n","                      'Zipcode_86553',\n","                      'Years of Experience', \n","                      'Years of Volunteering',\n","                      'High School GPA',\n","                      'College GPA',  \n","                      'English Fluency_Basic',\n","                      'English Fluency_Fluent',\n","                      'English Fluency_Proficient',\n","                      'Spanish Fluency_Basic',\n","                      'Spanish Fluency_Fluent',\n","                      'Spanish Fluency_Proficient',\n","                      'Education_Associates',\n","                      'Education_Graduate',\n","                      'Education_Undergraduate',\n","                      'MBTI_EI_E',\n","                      'MBTI_EI_I',\n","                      'MBTI_SN_N',\n","                      'MBTI_SN_S',\n","                      'MBTI_TF_F',\n","                      'MBTI_TF_T',\n","                      'MBTI_JP_J',\n","                      'MBTI_JP_P',\n","                      'Requires Sponsorship_False',\n","                      'Requires Sponsorship_True',\n","                      ]"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Oe68bBEcQrkb","nbgrader":{"cell_type":"markdown","checksum":"a6b02cbd6f7418d4619aab8525689ad3","grade":false,"grade_id":"cell-b4b87681c2bdc21a","locked":true,"schema_version":3,"solution":false}},"source":["Why did you choose the features you did?"]},{"cell_type":"code","metadata":{"deletable":false,"id":"_aT9KBg0Qrkb","nbgrader":{"cell_type":"code","checksum":"24c40e656440a66d5997e9c87486940e","grade":true,"grade_id":"cell-b95231da655a979d","locked":true,"points":4,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1639086467089,"user_tz":420,"elapsed":117,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["## Save your reasoning in a string to the variable hire_reason\n","\n","# YOUR CODE HERE\n","hire_reason = '''In general, I would imagine if money/urgency were factors that something like sponsorship for citizenship or something could also be important. I also thought some location info would be important (zip code). \n"," Primarily just to avoid having a lag due to relocating an employee, or something like this. The other features are the same! '''"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"WOr6CM_tQrke","nbgrader":{"cell_type":"code","checksum":"143532aecc0e98542ad8d073decf1a57","grade":true,"grade_id":"cell-34d2462a6b2f374c","locked":true,"points":2,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086469107,"user_tz":420,"elapsed":173,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert isinstance(hire_reason, str)\n","assert len(hire_reason) > 20"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"uKIkN6t-Qrkg","nbgrader":{"cell_type":"markdown","checksum":"517eec5e5592eae3ea6c7ff7c4d63d17","grade":false,"grade_id":"cell-5d3c9cf0fe1dac76","locked":true,"schema_version":3,"solution":false}},"source":["Why was your choice different from or the same as the interviewing features?\n"]},{"cell_type":"code","metadata":{"deletable":false,"id":"9NAvxqfvQrkg","nbgrader":{"cell_type":"code","checksum":"bfcaac14949eab25c9738f2c72d2b07d","grade":true,"grade_id":"cell-4479d690eeb487ad","locked":true,"points":4,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086472482,"user_tz":420,"elapsed":138,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"e1a9ab10-2203-47e5-8a6a-0fe5729d7ee4"},"source":["# Save your reasoning in a string to the variable\n","# \"same_reason\" if the features are the same, or\n","# \"different_reason\" if the features are different.\n","\n","# YOUR CODE HERE\n","different_reason = hire_reason\n","if 'same_reason' in locals():\n","    print(same_reason)\n","else:\n","    print(different_reason)"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["In general, I would imagine if money/urgency were factors that something like sponsorship for citizenship or something could also be important. I also thought some location info would be important (zip code). \n"," Primarily just to avoid having a lag due to relocating an employee, or something like this. The other features are the same! \n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"rErewAYqQrkl","nbgrader":{"cell_type":"code","checksum":"17aa34332695be8cf2a84d3c5a4815ed","grade":true,"grade_id":"cell-f875ee0056d6eb98","locked":true,"points":2,"schema_version":3,"solution":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086474103,"user_tz":420,"elapsed":314,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"f9585e6b-6ade-47c2-c53e-6f407ac7a76c"},"source":["if all([rf in hire_features for rf in interview_features]) and all([sf in interview_features for sf in hire_features]):\n","    print(\"Your features for interviewing and hiring are the same.\")\n","    assert isinstance(same_reason, str)\n","    assert len(same_reason) > 20\n","else:\n","    print(\"Your features for interviewing and hiring are different.\")\n","    assert isinstance(different_reason, str)\n","    assert len(different_reason) > 20"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Your features for interviewing and hiring are different.\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"id":"VbuVq9gsQrkn","nbgrader":{"cell_type":"code","checksum":"cd12f6bd1b62fafcdf3014264bf5902c","grade":false,"grade_id":"cell-fe98928b7423b02b","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086475357,"user_tz":420,"elapsed":137,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Perform a train and test split on the data with the variable names:\n","#\n","# hire_x_train for the training features\n","# hire_x_test  for the testing features\n","#\n","# hire_y_train for the training targets\n","# hire_y_test  for the testing targets\n","#\n","# The test dataset should be 20% of the total dataset\n","\n","# YOUR CODE HERE\n","hire_x_train, hire_x_test, hire_y_train, hire_y_test = train_test_split(df[hire_features], df[targets], test_size=0.2)"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"pmSR0X0uQrkq","nbgrader":{"cell_type":"code","checksum":"93eff4df6e01750e0ee6a7bd35f0ea79","grade":true,"grade_id":"cell-be29120b1249cd74","locked":true,"points":2,"schema_version":3,"solution":false},"executionInfo":{"status":"ok","timestamp":1639086476713,"user_tz":420,"elapsed":138,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert (len(hire_x_train) / (len(hire_x_test) + len(hire_x_train))) == 0.8\n","assert (len(hire_y_train) / (len(hire_y_test) + len(hire_y_train))) == 0.8\n","assert len(hire_x_train) == len(hire_y_train)\n","assert len(hire_x_test) == len(hire_y_test)"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"lbc63m_SQrks","nbgrader":{"cell_type":"markdown","checksum":"101ed3dcd7e1ae4166adfe3b64f5919c","grade":false,"grade_id":"cell-3ae20ccdd5d080b7","locked":true,"schema_version":3,"solution":false}},"source":["Build and train your hiring models.\n","\n","Do you expect this model to perform differently?"]},{"cell_type":"code","metadata":{"deletable":false,"id":"u9vuP3ulQrkt","nbgrader":{"cell_type":"code","checksum":"822844e5ccdbc299c0c4324eec25c1c0","grade":true,"grade_id":"cell-43f1427eb940e017","locked":true,"points":10,"schema_version":3,"solution":true},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086478988,"user_tz":420,"elapsed":804,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"52cae957-af00-44ad-cadc-633c2db52625"},"source":["# Select models of your choosing, import them here, and perform a\n","# hyperparameter search while training them on each of the targets.\n","#\n","# Do not use Tensorflow to build a model - you may use scikit-learn.\n","#\n","# Determine an appropriate metric for measuring your performance for each\n","# model/target, and report the test score for that metric. The metric may be\n","# different for each model/target.\n","#\n","# Save your models in a list, with models ordered in the same manner as the\n","# targets they are predicting in the list \"targets\" you created above.\n","# Call the list \"my_hiring_models\", e.g.\n","#\n","#    my_hiring_models = [hiring_model_target1,\n","#                        hiring_model_target2,\n","#                        hiring_model_target3]\n","#\n","# You should use multiple print messages to print something like the\n","# following for each of your models/targets:\n","#\n","#    To predict the target (target), I trained a (model) model\n","#    and determined the best hyperparameters as (param1 = p1), (param2 = p2), ...\n","#    resulting in a (metric) score of (score).\n","\n","# YOUR CODE HERE\n","# My initial thoughts are to just use a ridge model for all 3.\n","model = Ridge()\n","\n","parameters = {'alpha':[1, 10]}\n","hire_scores = []\n","my_hiring_models = []\n","\n","for t in targets:\n","  # search & fit\n","  search = GridSearchCV(model, parameters, scoring='neg_mean_squared_error', cv=5)\n","  search.fit(hire_x_train, hire_y_train[t])\n","  # MSE calculations + print statement. \n","  best_esti = search.best_estimator_\n","  pred = best_esti.predict(hire_x_test)\n","  mse = mean_squared_error(hire_y_test[t], pred)\n","  hire_scores.append(mse)\n","  my_hiring_models.append(best_esti)\n","  output = output = 'To predict the target, Customer Satisfaction Rating, I trained a Ridge model and determined the best hyperparameter to be alpha = {}, resulting in an MSE score of {}'.format(search.best_estimator_, mse)\n","  print(output)\n","\n","# List of models to use in the skeleton code\n","print(len(my_hiring_models))"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["To predict the target, Customer Satisfaction Rating, I trained a Ridge model and determined the best hyperparameter to be alpha = Ridge(alpha=1), resulting in an MSE score of 0.0008172086489044972\n","To predict the target, Customer Satisfaction Rating, I trained a Ridge model and determined the best hyperparameter to be alpha = Ridge(alpha=1), resulting in an MSE score of 0.0005965592488081609\n","To predict the target, Customer Satisfaction Rating, I trained a Ridge model and determined the best hyperparameter to be alpha = Ridge(alpha=10), resulting in an MSE score of 0.06600747555455964\n","3\n"]}]},{"cell_type":"code","metadata":{"id":"RCxIICXkyZu-","executionInfo":{"status":"ok","timestamp":1639086481266,"user_tz":420,"elapsed":132,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(my_hiring_models)==len(targets)"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"21INaL-DQrkv","nbgrader":{"cell_type":"code","checksum":"4b75617f8511a0aec1b3c653c0f4121a","grade":true,"grade_id":"cell-5df6d98d3b874670","locked":true,"points":4,"schema_version":3,"solution":true},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086482368,"user_tz":420,"elapsed":122,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"d9b7c3f2-6164-4d64-aad1-955d73621879"},"source":["# Follow this up with a comparison between the performance (test set scores) on your\n","# two sets of models (six models in total).\n","#\n","# You should print something like, for each of the targets:\n","#\n","#   Using interview features for target (target) the model scored (score)\n","#   versus using the hiring features where it scored (score).\n","\n","# YOUR CODE HERE\n","for i,t in enumerate(targets):\n","  out = 'Using interview features for target: {} the model scored: {}, versus using the hiring features where it scored: {}'.format(t, interview_scores[i], hire_scores[i])\n","  print(out)"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Using interview features for target: Customer Satisfaction Rating the model scored: 0.0008721788657854672, versus using the hiring features where it scored: 0.0008172086489044972\n","Using interview features for target: Sales Rating the model scored: 0.0007203030444323621, versus using the hiring features where it scored: 0.0005965592488081609\n","Using interview features for target: Fired_Fired the model scored: 0.07678650013077483, versus using the hiring features where it scored: 0.06600747555455964\n"]}]},{"cell_type":"markdown","metadata":{"id":"cOB_29ZQDa5r"},"source":["## Model Evaluation\n","\n","In this section we'll create example applicants and see how they would fare based on their applications and your models. First, let's create some example applications. We've created four applicants, and you'll need to create a fifth one in the cell below."]},{"cell_type":"code","metadata":{"deletable":false,"id":"I_mNuuafD1ID","nbgrader":{"cell_type":"code","checksum":"242d9f1e013fbbace2a0afb07ac5910d","grade":false,"grade_id":"cell-f8b4174a433caec6","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1639086484720,"user_tz":420,"elapsed":117,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["applicant_1 = {\n","    'First Name': \"Stefon\",\n","    'Last Name': \"Smith\",\n","    'Date of Birth': \"1989-12-24\",\n","    'Address': \"4892 Jessica Turnpike Suite 781\",\n","    'Zipcode': 86553,\n","    'Gender': \"Male\",\n","    'Race / Ethnicity': \"Caucasian\",\n","    'English Fluency': \"Proficient\",\n","    'Spanish Fluency': \"Basic\",\n","    'Education': \"Associates\",\n","    'High School GPA': 2.9,\n","    'College GPA': 3.1,\n","    'Years of Experience': 5,\n","    'Years of Volunteering': 2,\n","    'Myers Briggs Type': \"ESFJ\",\n","    'Twitter followers': 524,\n","    'Instagram Followers': 857,\n","    'Requires Sponsorship': True\n","}\n","applicant_2 = {\n","    'First Name': \"Sarah\",\n","    'Last Name': \"Chang\",\n","    'Date of Birth': \"1995-04-13\",\n","    'Address': \"9163 Rebecca Loop\",\n","    'Zipcode': 43711,\n","    'Gender': \"Female\",\n","    'Race / Ethnicity': \"Hispanic\",\n","    'English Fluency': \"Fluent\",\n","    'Spanish Fluency': \"Fluent\",\n","    'Education': \"Undergraduate\",\n","    'High School GPA': 4.0,\n","    'College GPA': 3.8,\n","    'Years of Experience': 5,\n","    'Years of Volunteering': 0,\n","    'Myers Briggs Type': \"ISTJ\",\n","    'Twitter followers': 97,\n","    'Instagram Followers': 204,\n","    'Requires Sponsorship': False\n","}\n","applicant_3 = {\n","    'First Name': \"Daniel\",\n","    'Last Name': \"Richardson\",\n","    'Date of Birth': \"1998-10-23\",\n","    'Address': \"436 Lauren Stream\",\n","    'Zipcode': 54821,\n","    'Gender': \"Male\",\n","    'Race / Ethnicity': \"Black\",\n","    'English Fluency': \"Fluent\",\n","    'Spanish Fluency': \"Proficient\",\n","    'Education': \"Undergraduate\",\n","    'High School GPA': 3.0,\n","    'College GPA': 3.2,\n","    'Years of Experience': 1,\n","    'Years of Volunteering': 1,\n","    'Myers Briggs Type': \"ENFJ\",\n","    'Twitter followers': 2087,\n","    'Instagram Followers': 3211,\n","    'Requires Sponsorship': False\n","}\n","\n","applicant_4 = {\n","    'First Name': \"Billy\",\n","    'Last Name': \"Bob\",\n","    'Date of Birth': \"1999-11-03\",\n","    'Address': \"412 Railway Stream\",\n","    'Zipcode': 43711,\n","    'Gender': \"Male\",\n","    'Race / Ethnicity': \"Caucasian\",\n","    'English Fluency': \"Basic\",\n","    'Spanish Fluency': \"Fluent\",\n","    'Education': \"Undergraduate\",\n","    'High School GPA': 2.0,\n","    'College GPA': 3.5,\n","    'Years of Experience': 1,\n","    'Years of Volunteering': 1,\n","    'Myers Briggs Type': \"ENFJ\",\n","    'Twitter followers': 207,\n","    'Instagram Followers': 309,\n","    'Requires Sponsorship': False\n","}\n","\n","# Create a fictional applicant by copying the attributes above from any of the\n","# other applicants and altering values that you would be curious to\n","# see how your model treats. For example, create an applicant you'd (not your\n","# model) be sure to reject or sure to hire.\n","\n","# YOUR CODE HERE\n","applicant_5 = {\n","    'First Name': \"Ricky\",\n","    'Last Name': \"Bobby\",\n","    'Date of Birth': \"1996-01-01\",\n","    'Address': \"542 Reindeer Road\",\n","    'Zipcode': 43711,\n","    'Gender': \"Male\",\n","    'Race / Ethnicity': \"Caucasian\",\n","    'English Fluency': \"Fluent\",\n","    'Spanish Fluency': \"Fluent\",\n","    'Education': \"Graduate\",\n","    'High School GPA': 3.0,\n","    'College GPA': 3.1,\n","    'Years of Experience': 10,\n","    'Years of Volunteering': 6,\n","    'Myers Briggs Type': \"ENFP\",\n","    'Twitter followers': 321,\n","    'Instagram Followers': 123,\n","    'Requires Sponsorship': False\n","}"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"VvWLYqtEUE9q","nbgrader":{"cell_type":"code","checksum":"9c7a280b61aea8b1ef7fd504645bfd2f","grade":true,"grade_id":"cell-5a2332a45ba35b42","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086489222,"user_tz":420,"elapsed":136,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["for key in applicant_4.keys():\n","    assert key in applicant_5.keys()"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXqD3wgDD1PX","nbgrader":{"grade":false,"grade_id":"cell-2cf93863baeae2d9","locked":false,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086490024,"user_tz":420,"elapsed":132,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["new_people = [applicant_1, applicant_2, applicant_3, applicant_4, applicant_5]\n","new_people_df = pd.DataFrame.from_records(new_people)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Q50H8SnhRu8","colab":{"base_uri":"https://localhost:8080/","height":414},"executionInfo":{"status":"ok","timestamp":1639086491033,"user_tz":420,"elapsed":139,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"6c6c9350-0422-4c94-e25d-74705344e883"},"source":["new_people_df"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>First Name</th>\n","      <th>Last Name</th>\n","      <th>Date of Birth</th>\n","      <th>Address</th>\n","      <th>Zipcode</th>\n","      <th>Gender</th>\n","      <th>Race / Ethnicity</th>\n","      <th>English Fluency</th>\n","      <th>Spanish Fluency</th>\n","      <th>Education</th>\n","      <th>High School GPA</th>\n","      <th>College GPA</th>\n","      <th>Years of Experience</th>\n","      <th>Years of Volunteering</th>\n","      <th>Myers Briggs Type</th>\n","      <th>Twitter followers</th>\n","      <th>Instagram Followers</th>\n","      <th>Requires Sponsorship</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stefon</td>\n","      <td>Smith</td>\n","      <td>1989-12-24</td>\n","      <td>4892 Jessica Turnpike Suite 781</td>\n","      <td>86553</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>Proficient</td>\n","      <td>Basic</td>\n","      <td>Associates</td>\n","      <td>2.9</td>\n","      <td>3.1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>ESFJ</td>\n","      <td>524</td>\n","      <td>857</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sarah</td>\n","      <td>Chang</td>\n","      <td>1995-04-13</td>\n","      <td>9163 Rebecca Loop</td>\n","      <td>43711</td>\n","      <td>Female</td>\n","      <td>Hispanic</td>\n","      <td>Fluent</td>\n","      <td>Fluent</td>\n","      <td>Undergraduate</td>\n","      <td>4.0</td>\n","      <td>3.8</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>ISTJ</td>\n","      <td>97</td>\n","      <td>204</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Daniel</td>\n","      <td>Richardson</td>\n","      <td>1998-10-23</td>\n","      <td>436 Lauren Stream</td>\n","      <td>54821</td>\n","      <td>Male</td>\n","      <td>Black</td>\n","      <td>Fluent</td>\n","      <td>Proficient</td>\n","      <td>Undergraduate</td>\n","      <td>3.0</td>\n","      <td>3.2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>ENFJ</td>\n","      <td>2087</td>\n","      <td>3211</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Billy</td>\n","      <td>Bob</td>\n","      <td>1999-11-03</td>\n","      <td>412 Railway Stream</td>\n","      <td>43711</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>Basic</td>\n","      <td>Fluent</td>\n","      <td>Undergraduate</td>\n","      <td>2.0</td>\n","      <td>3.5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>ENFJ</td>\n","      <td>207</td>\n","      <td>309</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ricky</td>\n","      <td>Bobby</td>\n","      <td>1996-01-01</td>\n","      <td>542 Reindeer Road</td>\n","      <td>43711</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>Fluent</td>\n","      <td>Fluent</td>\n","      <td>Graduate</td>\n","      <td>3.0</td>\n","      <td>3.1</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>ENFP</td>\n","      <td>321</td>\n","      <td>123</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  First Name   Last Name  ... Instagram Followers Requires Sponsorship\n","0     Stefon       Smith  ...                 857                 True\n","1      Sarah       Chang  ...                 204                False\n","2     Daniel  Richardson  ...                3211                False\n","3      Billy         Bob  ...                 309                False\n","4      Ricky       Bobby  ...                 123                False\n","\n","[5 rows x 18 columns]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"7izcLclbHjNA"},"source":["### Future Applicants Data Cleaning\n","\n"]},{"cell_type":"code","metadata":{"deletable":false,"id":"Ue9L02lED1Xn","nbgrader":{"cell_type":"code","checksum":"823bca69f4da6cc87e4800ff6d74e8f3","grade":false,"grade_id":"cell-e8d9072c79ce7dab","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1639086492951,"user_tz":420,"elapsed":120,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Apply all the cleaning and dummy variable creation you did above to this new\n","# DataFrame. You can copy your code from above and modify it to apply to\n","# new_people_df instead of df.\n","\n","# YOUR CODE HERE\n","# explicitly add in the area codes from test-data (?) // I had errors with these because some of them (obviously) are not found in the applicants we create above... IDK if this is the right\n","# approach. \n","zipcodes =           ['Zipcode_24310',\n","                      'Zipcode_30167',\n","                      'Zipcode_43357',\n","                      'Zipcode_55864',\n","                      'Zipcode_59010',\n","                      'Zipcode_60531',\n","                      'Zipcode_72361'\n","                      ]\n","new_people_df[zipcodes] = 0\n","# Splits Myers Briggers\n","new_people_df[['MBTI_EI', 'MBTI_SN', 'MBTI_TF', 'MBTI_JP']] = new_people_df['Myers Briggs Type'].apply(lambda x: pd.Series(list(x)))\n","new_people_df = new_people_df.drop(columns='Myers Briggs Type')\n","# Categorical stuff\n","categorical_columns2 = ['Zipcode', 'Gender', 'Race / Ethnicity', 'English Fluency', 'Spanish Fluency', 'Education', 'MBTI_EI', 'MBTI_SN', 'MBTI_TF', 'MBTI_JP', 'Requires Sponsorship']\n","\n","new_people_df = pd.get_dummies(new_people_df, columns=categorical_columns2)\n","# Age stuff\n","new_people_df['Date of Birth'] = pd.to_datetime(new_people_df['Date of Birth'])\n","new_people_df[['Age']] = new_people_df['Date of Birth'].apply(lambda x: calculate_age(x))\n","new_people_df = new_people_df.drop(columns='Date of Birth')\n","# Handle NANs \n","nan_columns = new_people_df.columns[new_people_df.isna().any()].tolist()\n","new_people_df['High School GPA'] = new_people_df['High School GPA'].fillna(0)\n","new_people_df['College GPA'] = new_people_df['College GPA'].fillna(0)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"vh8eVaGmOkNB","colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"status":"ok","timestamp":1639086494688,"user_tz":420,"elapsed":375,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"f1295c22-9132-467b-9915-fff8d9f46747"},"source":["new_people_df"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>First Name</th>\n","      <th>Last Name</th>\n","      <th>Address</th>\n","      <th>High School GPA</th>\n","      <th>College GPA</th>\n","      <th>Years of Experience</th>\n","      <th>Years of Volunteering</th>\n","      <th>Twitter followers</th>\n","      <th>Instagram Followers</th>\n","      <th>Zipcode_24310</th>\n","      <th>Zipcode_30167</th>\n","      <th>Zipcode_43357</th>\n","      <th>Zipcode_55864</th>\n","      <th>Zipcode_59010</th>\n","      <th>Zipcode_60531</th>\n","      <th>Zipcode_72361</th>\n","      <th>Zipcode_43711</th>\n","      <th>Zipcode_54821</th>\n","      <th>Zipcode_86553</th>\n","      <th>Gender_Female</th>\n","      <th>Gender_Male</th>\n","      <th>Race / Ethnicity_Black</th>\n","      <th>Race / Ethnicity_Caucasian</th>\n","      <th>Race / Ethnicity_Hispanic</th>\n","      <th>English Fluency_Basic</th>\n","      <th>English Fluency_Fluent</th>\n","      <th>English Fluency_Proficient</th>\n","      <th>Spanish Fluency_Basic</th>\n","      <th>Spanish Fluency_Fluent</th>\n","      <th>Spanish Fluency_Proficient</th>\n","      <th>Education_Associates</th>\n","      <th>Education_Graduate</th>\n","      <th>Education_Undergraduate</th>\n","      <th>MBTI_EI_E</th>\n","      <th>MBTI_EI_I</th>\n","      <th>MBTI_SN_N</th>\n","      <th>MBTI_SN_S</th>\n","      <th>MBTI_TF_F</th>\n","      <th>MBTI_TF_T</th>\n","      <th>MBTI_JP_J</th>\n","      <th>MBTI_JP_P</th>\n","      <th>Requires Sponsorship_False</th>\n","      <th>Requires Sponsorship_True</th>\n","      <th>Age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stefon</td>\n","      <td>Smith</td>\n","      <td>4892 Jessica Turnpike Suite 781</td>\n","      <td>2.9</td>\n","      <td>3.1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>524</td>\n","      <td>857</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sarah</td>\n","      <td>Chang</td>\n","      <td>9163 Rebecca Loop</td>\n","      <td>4.0</td>\n","      <td>3.8</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>97</td>\n","      <td>204</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Daniel</td>\n","      <td>Richardson</td>\n","      <td>436 Lauren Stream</td>\n","      <td>3.0</td>\n","      <td>3.2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2087</td>\n","      <td>3211</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Billy</td>\n","      <td>Bob</td>\n","      <td>412 Railway Stream</td>\n","      <td>2.0</td>\n","      <td>3.5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>207</td>\n","      <td>309</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ricky</td>\n","      <td>Bobby</td>\n","      <td>542 Reindeer Road</td>\n","      <td>3.0</td>\n","      <td>3.1</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>321</td>\n","      <td>123</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  First Name   Last Name  ... Requires Sponsorship_True  Age\n","0     Stefon       Smith  ...                         1   31\n","1      Sarah       Chang  ...                         0   26\n","2     Daniel  Richardson  ...                         0   23\n","3      Billy         Bob  ...                         0   22\n","4      Ricky       Bobby  ...                         0   25\n","\n","[5 rows x 44 columns]"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"fp8p3gdiD1NN","nbgrader":{"cell_type":"code","checksum":"165454e1993917d81fb9fcf239dbdf62","grade":true,"grade_id":"cell-f9226b28d96f00a8","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086496913,"user_tz":420,"elapsed":131,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["for feature in interview_features:\n","    assert feature in new_people_df.columns\n","for feature in hire_features:\n","    assert feature in new_people_df.columns"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUfDigqdHir7"},"source":["### Future Applicant Model(s) Predictions\n","\n","Now use your `my_interview_models` and your `my_hiring_models` to predict applicant scores, and store those predictions."]},{"cell_type":"code","metadata":{"deletable":false,"id":"yDbBZnIvHTGq","nbgrader":{"cell_type":"code","checksum":"f0fb4df3635d86cde60ba57652479eb0","grade":false,"grade_id":"cell-8ecd71076203ea0e","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086499063,"user_tz":420,"elapsed":189,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"1cbce283-fe7a-479b-cd8a-a2237c06246f"},"source":["# Use your models to make predictions for people in new_people_df.\n","#\n","# Save your predictions as a \"new_people_interview_pred\" list and \n","# a \"new_people_hire_pred\" list. Each should be a list of five dictionaries\n","# (one for each applicant). The keys of the dictionaries should be the\n","# same as the strings in the \"targets\" list you created above.\n","\n","# YOUR CODE HERE\n","# To save our eyes... I couldn't for the life of me find the correct combination of .values to fit/train the data to avoid the Warnings.... so I will hide them :D\n","#   - The warning is just saying that the data was train with column names and fit without them, but changing the obvious things broke the code in other spots :(\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","new_people_interview_pred = []\n","new_people_hire_pred = []\n","# the keys of the dictionary are just targets[]... and the value is the score from that model...\n","# then we append this dictionary to the lists above...\n","#   - boils down to having a list with 5 entry, each entry has 3 measurements stored in a dictionary.\n","\n","for i in range(len(new_people_df)):\n","  # keys for making the dictionary\n","  keys = targets\n","  # 2 empty lists to populate the dictionary values \n","  interview_res = []\n","  hire_res = []\n","  for j in range(3):\n","    # interview\n","    person = new_people_df.iloc[i]\n","    interview_res.append(my_interview_models[j].predict(person[interview_features].to_numpy().reshape(1,-1)))\n","    # interview_satisfaction_val, interview_sales_val, interview_fired_val = my_interview_models[0].predict(new_people_df[i]), my_interview_models[1].predict(new_people_df[i]), my_interview_models[2].predict(new_people_df[i])\n","    # hire\n","    hire_res.append(my_hiring_models[j].predict(person[hire_features].to_numpy().reshape(1,-1)))\n","    # hire_satisfaction_val, hire_sales_val, hire_fired_val = my_hiring_models[0].predict(new_people_df[i]), my_hiring_models[1].predict(new_people_df[i]), my_hiring_models[2].predict(new_people_df[i])\n","  interview_dict = {keys[z]: interview_res[z] for z in range(3)}\n","  hiring_dict = {keys[z]: hire_res[z] for z in range(3)}\n","  new_people_interview_pred.append(interview_dict)\n","  new_people_hire_pred.append(hiring_dict)\n","\n","print('Interview models:')\n","for p in new_people_interview_pred:\n","    print(p)\n","\n","print('\\nHiring models:')\n","for p in new_people_hire_pred:\n","    print(p)"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Interview models:\n","{'Customer Satisfaction Rating': array([2.06369628]), 'Sales Rating': array([1.88096889]), 'Fired_Fired': array([0.07535652])}\n","{'Customer Satisfaction Rating': array([1.7680669]), 'Sales Rating': array([1.93027125]), 'Fired_Fired': array([0.06343208])}\n","{'Customer Satisfaction Rating': array([1.29552187]), 'Sales Rating': array([1.337515]), 'Fired_Fired': array([0.1215484])}\n","{'Customer Satisfaction Rating': array([0.98947136]), 'Sales Rating': array([1.0858942]), 'Fired_Fired': array([0.17336368])}\n","{'Customer Satisfaction Rating': array([4.44640278]), 'Sales Rating': array([4.30720478]), 'Fired_Fired': array([-0.05301207])}\n","\n","Hiring models:\n","{'Customer Satisfaction Rating': array([2.05522462]), 'Sales Rating': array([1.91326915]), 'Fired_Fired': array([0.13608429])}\n","{'Customer Satisfaction Rating': array([1.76228811]), 'Sales Rating': array([1.92785851]), 'Fired_Fired': array([0.06601563])}\n","{'Customer Satisfaction Rating': array([1.30183634]), 'Sales Rating': array([1.33344122]), 'Fired_Fired': array([0.05329685])}\n","{'Customer Satisfaction Rating': array([0.9891868]), 'Sales Rating': array([1.08587425]), 'Fired_Fired': array([0.14458877])}\n","{'Customer Satisfaction Rating': array([4.4291264]), 'Sales Rating': array([4.29667476]), 'Fired_Fired': array([-0.06896651])}\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"rooCaeaeHzjX","nbgrader":{"cell_type":"code","checksum":"07dda40041447615a952d70ceb63bac8","grade":true,"grade_id":"cell-6c696e5279f751cc","locked":true,"points":6,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086501571,"user_tz":420,"elapsed":116,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["for person in new_people_interview_pred:\n","    for key in targets:\n","        assert key in person.keys()\n","\n","for person in new_people_hire_pred:\n","    for key in targets:\n","        assert key in person.keys()"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enVnHJUsIoso"},"source":["### Ranking Evaluation\n","\n","Your manager notes that given that you have more than one prediction target, the model predictions aren't really ranking or selecting people. There is no \"best\" person because there's more than one metric to look through. A human still needs to look at all three predictions so your models don't yet really do what SellsALOT has asked for.\n","\n","Your manager asks you to create a synthetic scalar variable that is calculated from the multiple target predictions of an individual person. That way we'll have one metric by which we can rank people. You need to create that synthetic metric (score).\n","\n","Some candidate approaches:\n","\n","1. Incorporating a binary value (such as fired/not-fired), x:\n","    - You can multiply x by some arbitrary value and add/subtract it to/from the total score:\n","      - score = t1 + t2 * x\n","    - You can multiple your entire score output by the binary value to say something like \"if not x, then  score is 0\", e.g.:\n","      - score = x * (t1 + t2)\n","1. Weighting different target values:\n","    - You can weight different values by adding a multiplier (if t1 is twice as important as t2, then the score can be something like:\n","     - score = 2 * t1 + t2\n","1. Some combination of the items above\n","1. Something creative you devise on your own!"]},{"cell_type":"code","metadata":{"deletable":false,"id":"-PJoR68cInMi","nbgrader":{"cell_type":"code","checksum":"a20a434440cbaa8f2424bdc7bef7412d","grade":false,"grade_id":"cell-d639d76def9446d0","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1639086504489,"user_tz":420,"elapsed":151,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["def calculate_synthetic_metric(targets):\n","    \"\"\"Calculates a synthetic matric based on the targets of an individual\n","    Your metric should result in a higher score being a better one\n","\n","    Args:\n","      targets (dict): The dictionary with keys as the target names and\n","                      values as the target values/predictions\n","\n","    Returns:\n","      float: The synthetic score produced from \n","    \"\"\"\n","    # YOUR CODE HERE\n","    res = (2 * targets['Sales Rating'] + targets['Customer Satisfaction Rating']) - 10 * targets['Fired_Fired']\n","    return float(res)"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A658jUL-Kxpu"},"source":["Let's try out the synthetic metric on the original data and see if you're happy with the result based on the past data."]},{"cell_type":"code","metadata":{"deletable":false,"id":"RaTPe6B8Ksho","nbgrader":{"cell_type":"code","checksum":"784017595af02177f2e71bea56326605","grade":false,"grade_id":"cell-3d6e8af884146387","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1639086511887,"user_tz":420,"elapsed":5610,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["# Add a column named \"Metric\" to the *original* DataFrame (\"df\") with the\n","# synthetic metric applied to each row.\n","\n","# YOUR CODE HERE\n","# print(new_people_df)\n","# # the code works with the calculated dictionaries above... but how do I apply this function to the rows of the original dataframe?\n","# for p in new_people_interview_pred:\n","#   print(calculate_synthetic_metric(p))\n","# I'll just do what I did to the new_people_df to the original df and then merge the 2... this is probably not the IDEAL solution but it should work nevertheless...\n","synthetic_scores = []\n","for i in range(len(df)):\n","  keys = targets\n","  # I'll just default ot  hiring rather than interview, as the latest scenario we are given is the one in which the company is directly hiring candidates. \n","  res = []\n","  for j in range(3):\n","    person = df.iloc[i]\n","    res.append(my_hiring_models[j].predict(person[hire_features].to_numpy().reshape(1,-1)))\n","  new_dict = {keys[z]: res[z] for z in range(3)}\n","  synthetic_scores.append(calculate_synthetic_metric(new_dict))\n","\n","# create column\n","df['Metric'] = synthetic_scores"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcnVkFH0LJs5","colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"status":"ok","timestamp":1639086514365,"user_tz":420,"elapsed":131,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"58f558bd-94b8-4a63-a1dd-303171158b59"},"source":["df.head()"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>First Name</th>\n","      <th>Last Name</th>\n","      <th>Address</th>\n","      <th>High School GPA</th>\n","      <th>College GPA</th>\n","      <th>Years of Experience</th>\n","      <th>Years of Volunteering</th>\n","      <th>Twitter followers</th>\n","      <th>Instagram Followers</th>\n","      <th>Customer Satisfaction Rating</th>\n","      <th>Sales Rating</th>\n","      <th>Zipcode_24310</th>\n","      <th>Zipcode_30167</th>\n","      <th>Zipcode_43357</th>\n","      <th>Zipcode_43711</th>\n","      <th>Zipcode_54821</th>\n","      <th>Zipcode_55864</th>\n","      <th>Zipcode_59010</th>\n","      <th>Zipcode_60531</th>\n","      <th>Zipcode_72361</th>\n","      <th>Zipcode_86553</th>\n","      <th>Gender_Female</th>\n","      <th>Gender_Male</th>\n","      <th>Race / Ethnicity_Black</th>\n","      <th>Race / Ethnicity_Caucasian</th>\n","      <th>Race / Ethnicity_Hispanic</th>\n","      <th>English Fluency_Basic</th>\n","      <th>English Fluency_Fluent</th>\n","      <th>English Fluency_Proficient</th>\n","      <th>Spanish Fluency_Basic</th>\n","      <th>Spanish Fluency_Fluent</th>\n","      <th>Spanish Fluency_Proficient</th>\n","      <th>Education_Associates</th>\n","      <th>Education_Graduate</th>\n","      <th>Education_High School</th>\n","      <th>Education_None</th>\n","      <th>Education_Undergraduate</th>\n","      <th>MBTI_EI_E</th>\n","      <th>MBTI_EI_I</th>\n","      <th>MBTI_SN_N</th>\n","      <th>MBTI_SN_S</th>\n","      <th>MBTI_TF_F</th>\n","      <th>MBTI_TF_T</th>\n","      <th>MBTI_JP_J</th>\n","      <th>MBTI_JP_P</th>\n","      <th>Requires Sponsorship_False</th>\n","      <th>Requires Sponsorship_True</th>\n","      <th>Fired_Current Employee</th>\n","      <th>Fired_Fired</th>\n","      <th>Age</th>\n","      <th>Metric</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sarah</td>\n","      <td>Chang</td>\n","      <td>764 Howard Tunnel</td>\n","      <td>3.10</td>\n","      <td>2.52</td>\n","      <td>8.8</td>\n","      <td>0.0</td>\n","      <td>693</td>\n","      <td>1108</td>\n","      <td>2.21</td>\n","      <td>2.07</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>31</td>\n","      <td>5.120133</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Daniel</td>\n","      <td>Taylor</td>\n","      <td>4892 Jessica Turnpike Suite 781</td>\n","      <td>3.02</td>\n","      <td>3.90</td>\n","      <td>13.7</td>\n","      <td>0.0</td>\n","      <td>507</td>\n","      <td>1259</td>\n","      <td>3.37</td>\n","      <td>2.98</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>36</td>\n","      <td>9.544119</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Heather</td>\n","      <td>Stewart</td>\n","      <td>778 Linda Orchard Apt. 609</td>\n","      <td>2.95</td>\n","      <td>2.63</td>\n","      <td>5.2</td>\n","      <td>0.0</td>\n","      <td>599</td>\n","      <td>868</td>\n","      <td>1.50</td>\n","      <td>1.36</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>28</td>\n","      <td>2.857821</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Katherine</td>\n","      <td>Dillon</td>\n","      <td>139 Linda Crossroad Suite 115</td>\n","      <td>3.99</td>\n","      <td>3.88</td>\n","      <td>12.5</td>\n","      <td>0.0</td>\n","      <td>1321</td>\n","      <td>889</td>\n","      <td>2.89</td>\n","      <td>2.62</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>34</td>\n","      <td>7.374188</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sheri</td>\n","      <td>Bolton</td>\n","      <td>1858 Lauren Orchard</td>\n","      <td>3.82</td>\n","      <td>3.30</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>414</td>\n","      <td>13760</td>\n","      <td>1.94</td>\n","      <td>1.78</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>4.900776</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  First Name Last Name  ... Age    Metric\n","0      Sarah     Chang  ...  31  5.120133\n","1     Daniel    Taylor  ...  36  9.544119\n","2    Heather   Stewart  ...  28  2.857821\n","3  Katherine    Dillon  ...  34  7.374188\n","4      Sheri    Bolton  ...  30  4.900776\n","\n","[5 rows x 51 columns]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"jztW-KZrIKSw","nbgrader":{"cell_type":"code","checksum":"f921ca690cd9ea24d91d4a542e4f3d0e","grade":true,"grade_id":"cell-bdb5b26a22302883","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086517622,"user_tz":420,"elapsed":137,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert \"Metric\" in df.columns\n","assert np.issubdtype(df[\"Metric\"].dtype, np.number)"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qy_LBAqeLQT0"},"source":["Are you happy with the synthetic score based on the values for each person here? Go back and update it until you're satisfied with this score."]},{"cell_type":"code","metadata":{"deletable":false,"id":"VaLgvQurLkjE","nbgrader":{"cell_type":"code","checksum":"e1cc28d6077d32a70de868f5baeb8686","grade":true,"grade_id":"cell-25c42dc943a39e64","locked":true,"points":4,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086519607,"user_tz":420,"elapsed":143,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"1bc7a66e-d7f4-4caf-f716-a6c7cd89be45"},"source":["# Explain the logic behind your synthetic scoring mechanism and\n","# save it as \"synthetic_score_reasoning\".\n","\n","# YOUR CODE HERE\n","synthetic_score_reasoning = '''In general, I figured that a sales company likely prioritizes sales/profit over everything. Therefore, I weighted sales twice as heavily as satisfaction.\n"," Also, I adjusted the score by subtracting off the score to be fired. More specifically, the weight of the fired modifier was 10 (5x that of sales). I thought this would provide enough\n"," variability to make the score meaningful, without having to do any adjustments (ie. normalization, etc).'''\n","print(synthetic_score_reasoning)"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["In general, I figured that a sales company likely prioritizes sales/profit over everything. Therefore, I weighted sales twice as heavily as satisfaction.\n"," Also, I adjusted the score by subtracting off the score to be fired. More specifically, the weight of the fired modifier was 10 (5x that of sales). I thought this would provide enough\n"," variability to make the score meaningful, without having to do any adjustments (ie. normalization, etc).\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"UlnqBpZUL1a8","nbgrader":{"cell_type":"code","checksum":"e26205e4ee274727d1b07fa779be3eff","grade":true,"grade_id":"cell-1fec797ad27734ef","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1639086520573,"user_tz":420,"elapsed":139,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(synthetic_score_reasoning) > 100"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uk-TMmydL7qn"},"source":["Now let's calculate the synthetic scores for the new people (applicants) and see if you're satisfied with your models' rankings for interviewing and hiring."]},{"cell_type":"code","metadata":{"id":"9eC4ydbKLPnk","executionInfo":{"status":"ok","timestamp":1639086522377,"user_tz":420,"elapsed":118,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["new_people_interview_score = [calculate_synthetic_metric(target_vals) for target_vals in new_people_interview_pred]\n","new_people_hire_score = [calculate_synthetic_metric(target_vals) for target_vals in new_people_hire_pred]"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHhfGnvTMh_T","executionInfo":{"status":"ok","timestamp":1639086527253,"user_tz":420,"elapsed":286,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["best_interview_person = new_people[new_people_interview_score.index(max(new_people_interview_score))]\n","best_hire_person = new_people[new_people_hire_score.index(max(new_people_hire_score))]"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nikv-vhUMi-z"},"source":["Based on these scores, your model selected the following people:"]},{"cell_type":"code","metadata":{"id":"JEoWB0MGNFS-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086529640,"user_tz":420,"elapsed":130,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"700ccafe-0e65-4b04-8f20-95de9ab59653"},"source":["print(f\"\"\"\n","Your interviewing model selected {best_interview_person['First Name']} {best_interview_person['Last Name']} as the person to interview.\n","\n","Your hiring model selected {best_hire_person['First Name']} {best_hire_person['Last Name']} as the person to hire.\n","\"\"\")"],"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Your interviewing model selected Ricky Bobby as the person to interview.\n","\n","Your hiring model selected Ricky Bobby as the person to hire.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"xyEJR9axNmv_"},"source":["Are you happy with these results? Feel free to modify the `applicant_5`'s attributes and see how your model performs based on changing these values. "]},{"cell_type":"code","metadata":{"deletable":false,"id":"FFWGYkmpNx_C","nbgrader":{"cell_type":"code","checksum":"b58192486c1d00b521d0a6602570a62e","grade":true,"grade_id":"cell-cc40d17ae80242e0","locked":true,"points":2,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639086532363,"user_tz":420,"elapsed":132,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}},"outputId":"af7f760b-08fb-4be6-c8ff-715384b5d0b8"},"source":["# Describe your level of satisfaction with your models.\n","# Did you edit your model based on the results? What did you change?\n","# What general conclusions did you get from the exercise?\n","# Save your answer to the above questions as \"conclusions\".\n","\n","# YOUR CODE HERE\n","conclusions = ''' I am pretty satisfied with this model! Looking at the few entries above, I was originally confused why Daniel Taylor and Katherine Dillon had such a discrepency in points, but\n","one of them does have slightly higher sales/customer satisfaction metric. Further, the need for sponsorship impacts the result indirectly due to it being a feature in the hiring models, which I used\n","for these calculations. Further, I did edit the model based on the results, I mostly tinkered with the 5 people at the end to see what kind of decisions I could get the model to make. When I felt\n","they were at least somewhat valid I continued. I changed the weighting of the variables used in the metric calculations. The general conclusion I got from the exercise was that the model was able\n","to predict a candidates standing based on a number of different points of data. However, I think it also shows the need to have discussions, as these data points are not truly an accurate representation\n","of any candidates output, etc. One thing I did notice is that it often recommends the same person for hire/interview... Likely due to the similarly of features?'''\n","print(conclusions)"],"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":[" I am pretty satisfied with this model! Looking at the few entries above, I was originally confused why Daniel Taylor and Katherine Dillon had such a discrepency in points, but\n","one of them does have slightly higher sales/customer satisfaction metric. Further, the need for sponsorship impacts the result indirectly due to it being a feature in the hiring models, which I used\n","for these calculations. Further, I did edit the model based on the results, I mostly tinkered with the 5 people at the end to see what kind of decisions I could get the model to make. When I felt\n","they were at least somewhat valid I continued. I changed the weighting of the variables used in the metric calculations. The general conclusion I got from the exercise was that the model was able\n","to predict a candidates standing based on a number of different points of data. However, I think it also shows the need to have discussions, as these data points are not truly an accurate representation\n","of any candidates output, etc. One thing I did notice is that it often recommends the same person for hire/interview... Likely due to the similarly of features?\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"hUxjTaSFK9e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vBdn2XzOIhk","executionInfo":{"status":"ok","timestamp":1639086578469,"user_tz":420,"elapsed":274,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["assert len(conclusions) > 100"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"aEDK4ChDQrky","nbgrader":{"cell_type":"markdown","checksum":"03dcba3c87298cd428ec01f9a0075d6c","grade":false,"grade_id":"cell-4a3caccc18c17350","locked":true,"schema_version":3,"solution":false}},"source":["## Feedback"]},{"cell_type":"code","metadata":{"deletable":false,"id":"w-Yu33YPQrky","nbgrader":{"cell_type":"code","checksum":"687d8bbd0750d67bdc439a0a539f4563","grade":false,"grade_id":"cell-259e62fb9f936804","locked":false,"schema_version":3,"solution":true},"executionInfo":{"status":"ok","timestamp":1639086580328,"user_tz":420,"elapsed":199,"user":{"displayName":"Nicholas McClellan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06956386808011262879"}}},"source":["def feedback():\n","    \"\"\"Provide feedback on the contents of this exercise\n","    \n","    Returns:\n","        string\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return 'N/A, thanks a lot for the semester!'"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"2Tmx8n-9Qrk0","nbgrader":{"cell_type":"code","checksum":"a981d530666ab499761c90062b408a01","grade":true,"grade_id":"cell-865e5f961cfd020f","locked":true,"points":0,"schema_version":3,"solution":false}},"source":[""],"execution_count":null,"outputs":[]}]}